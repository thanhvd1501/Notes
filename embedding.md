# ğŸ§  TOÃ€N Táº¬P KIáº¾N THá»¨C EMBEDDING, VECTOR DATABASE & SEMANTIC SEARCH

### (Tá»« cÆ¡ báº£n Ä‘áº¿n xÃ¢y dá»±ng há»‡ thá»‘ng AI thá»±c táº¿ & phá»ng váº¥n ká»¹ thuáº­t)

---

## ğŸŒ± CHÆ¯Æ NG 1: Ná»€N Táº¢NG Vá»€ EMBEDDINGS

### ğŸ§© 1. Embedding lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:**
Embeddings lÃ  **biá»ƒu diá»…n sá»‘ (vector)** cá»§a vÄƒn báº£n, hÃ¬nh áº£nh, Ã¢m thanh... trong **khÃ´ng gian nhiá»u chiá»u** (vector space), trong Ä‘Ã³ cÃ¡c Ä‘iá»ƒm gáº§n nhau mang nghÄ©a tÆ°Æ¡ng tá»± nhau.

* VÃ­ dá»¥:

  * â€œcatâ€ vÃ  â€œdogâ€ â†’ gáº§n nhau trong vector space.
  * â€œcarâ€ vÃ  â€œappleâ€ â†’ xa nhau.

ğŸ‘‰ GiÃºp mÃ¡y tÃ­nh â€œhiá»ƒu nghÄ©aâ€ thay vÃ¬ chá»‰ â€œnhÃ¬n tá»«â€.

**MÃ´ hÃ¬nh táº¡o embedding phá»• biáº¿n:**

* OpenAI: `text-embedding-3-small`, `text-embedding-3-large`
* Sentence-BERT (SBERT)
* Instructor / E5 (Hugging Face)

---

### âš™ï¸ 2. Cáº¥u trÃºc vector Embedding

Má»—i vÄƒn báº£n â†’ 1 vector gá»“m **1536 giÃ¡ trá»‹ float** (vá»›i `text-embedding-3-small`).

VÃ­ dá»¥:

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-small",
    input="AI helps automate tasks"
)
print(len(response.data[0].embedding))  # 1536
```

* Má»—i pháº§n tá»­ â‰ˆ â€œtá»a Ä‘á»™â€ thá»ƒ hiá»‡n Ã½ nghÄ©a.
* Dá»¯ liá»‡u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c so sÃ¡nh báº±ng cÃ¡c hÃ m khoáº£ng cÃ¡ch.

---

### ğŸ“ 3. Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (Similarity Metrics)

#### âœ… Cosine Similarity

* Phá»• biáº¿n nháº¥t.
* TÃ­nh báº±ng cÃ´ng thá»©c:
  [
  \text{similarity} = \frac{A \cdot B}{||A|| \times ||B||}
  ]
  (TÃ­ch vÃ´ hÆ°á»›ng chia cho tÃ­ch Ä‘á»™ dÃ i vector)

GiÃ¡ trá»‹:

* 1 â†’ giá»‘ng há»‡t
* 0 â†’ khÃ¡c hoÃ n toÃ n

Python:

```python
from scipy.spatial import distance
similarity = 1 - distance.cosine(v1, v2)
```

---

### ğŸ’¡ 4. á»¨ng dá»¥ng cá»§a Embeddings

| á»¨ng dá»¥ng                                 | MÃ´ táº£                                                     |
| ---------------------------------------- | --------------------------------------------------------- |
| **Semantic Search**                      | TÃ¬m ná»™i dung â€œcÃ¹ng nghÄ©aâ€ thay vÃ¬ khá»›p tá»« khÃ³a            |
| **Recommendation**                       | Gá»£i Ã½ ná»™i dung tÆ°Æ¡ng tá»±                                   |
| **Classification**                       | GÃ¡n nhÃ£n cho vÄƒn báº£n dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng               |
| **Clustering**                           | Gom nhÃ³m vÄƒn báº£n tÆ°Æ¡ng tá»±                                 |
| **RAG (Retrieval-Augmented Generation)** | Láº¥y thÃ´ng tin ngá»¯ cáº£nh Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng tráº£ lá»i cá»§a LLM |

---

## ğŸ” CHÆ¯Æ NG 2: á»¨NG Dá»¤NG EMBEDDING TRONG AI

---

### ğŸ§­ A. SEMANTIC SEARCH (TÃ¬m kiáº¿m ngá»¯ nghÄ©a)

**Má»¥c tiÃªu:**
TÃ¬m cÃ¡c vÄƒn báº£n â€œgáº§n nghÄ©aâ€ nháº¥t vá»›i cÃ¢u truy váº¥n.

**Pipeline cÆ¡ báº£n:**

1. Embed toÃ n bá»™ vÄƒn báº£n dá»¯ liá»‡u.
2. Embed cÃ¢u truy váº¥n.
3. TÃ­nh cosine similarity giá»¯a query vÃ  má»—i vÄƒn báº£n.
4. Tráº£ vá» top N káº¿t quáº£ gáº§n nháº¥t.

**VÃ­ dá»¥:**

```python
from openai import OpenAI
from scipy.spatial import distance
import numpy as np

client = OpenAI()

# Embed 2 vÄƒn báº£n
docs = ["I love AI", "I like cooking pasta"]
embs = [client.embeddings.create(model="text-embedding-3-small", input=d).data[0].embedding for d in docs]

# Query
query_emb = client.embeddings.create(model="text-embedding-3-small", input="Artificial intelligence is great").data[0].embedding

# TÃ­nh similarity
similarities = [1 - distance.cosine(query_emb, e) for e in embs]
print(np.argmax(similarities))  # -> chá»‰ má»¥c vÄƒn báº£n gáº§n nháº¥t
```

---

### ğŸ§© B. RECOMMENDATION SYSTEM (Há»‡ gá»£i Ã½)

Giá»‘ng semantic search, nhÆ°ng:

* Query = embedding cá»§a **item ngÆ°á»i dÃ¹ng Ä‘Ã£ xem**.
* TÃ­nh similarity Ä‘á»ƒ gá»£i Ã½ cÃ¡c item khÃ¡c.

**Gá»£i Ã½ trÃªn nhiá»u hÃ nh vi:**

```python
import numpy as np
user_vector = np.mean([v1, v2, v3], axis=0)
```

â†’ Trung bÃ¬nh embedding cÃ¡c bÃ i viáº¿t Ä‘Ã£ xem â†’ tÃ¬m cÃ¡c item gáº§n nháº¥t.

---

### ğŸ§¾ C. CLASSIFICATION (PhÃ¢n loáº¡i khÃ´ng giÃ¡m sÃ¡t)

Zero-shot classification báº±ng Embedding.

**Ã tÆ°á»Ÿng:**

1. Embed cÃ¡c nhÃ£n (label).
2. Embed vÄƒn báº£n cáº§n phÃ¢n loáº¡i.
3. TÃ­nh khoáº£ng cÃ¡ch, gÃ¡n nhÃ£n gáº§n nháº¥t.

```python
labels = ["Tech", "Business", "Sports"]
label_embs = [embed(l) for l in labels]
article_emb = embed("AI startup raises $10M")

pred = np.argmin([distance.cosine(article_emb, le) for le in label_embs])
print(labels[pred])  # Tech
```

ğŸ’¬ *Phá»ng váº¥n:*

> â€œLÃ m sao phÃ¢n loáº¡i vÄƒn báº£n mÃ  khÃ´ng cÃ³ dá»¯ liá»‡u huáº¥n luyá»‡n?â€
> âœ… Tráº£ lá»i: â€œBáº±ng cÃ¡ch sá»­ dá»¥ng zero-shot classification dá»±a trÃªn Embeddings vÃ  cosine similarity.â€

---

## ğŸ—„ï¸ CHÆ¯Æ NG 3: VECTOR DATABASES

---

### ğŸ“˜ 1. Váº¥n Ä‘á» vá»›i cÃ¡ch tiáº¿p cáº­n cÅ©

* Má»—i embedding cÃ³ 1536 float â‰ˆ 13 KB.
* Vá»›i 100k vÄƒn báº£n â†’ >1 GB RAM chá»‰ Ä‘á»ƒ lÆ°u embedding.
* So sÃ¡nh tuyáº¿n tÃ­nh (O(n)) ráº¥t cháº­m.
  ğŸ’¡ â†’ Cáº§n giáº£i phÃ¡p lÆ°u & truy váº¥n tá»‘i Æ°u: **Vector Databases**.

---

### ğŸ§± 2. KhÃ¡i niá»‡m Vector Database

**Chá»©c nÄƒng:**

* LÆ°u trá»¯ vector embeddings + metadata.
* Cho phÃ©p truy váº¥n theo ngá»¯ nghÄ©a (similarity search).
* Tá»‘i Æ°u hÃ³a báº±ng index (HNSW, FAISSâ€¦).

**VÃ­ dá»¥:** ChromaDB, Pinecone, Weaviate, Milvus, Qdrant.

---

### âš™ï¸ 3. Kiáº¿n trÃºc cÆ¡ báº£n

| ThÃ nh pháº§n     | Vai trÃ²                           |
| -------------- | --------------------------------- |
| **Embeddings** | Vector cá»§a vÄƒn báº£n                |
| **Documents**  | Ná»™i dung nguá»“n                    |
| **Metadata**   | ThÃ´ng tin phá»¥ (ID, type, year...) |
| **Indexes**    | Cáº¥u trÃºc giÃºp tÃ¬m kiáº¿m nhanh      |

---

### ğŸ§© 4. CÃ i Ä‘áº·t ChromaDB

**CÃ i Ä‘áº·t:**

```bash
pip install chromadb openai
```

**Káº¿t ná»‘i DB:**

```python
import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.create_collection(
    name="articles",
    embedding_function=OpenAIEmbeddingFunction(model_name="text-embedding-3-small", api_key="...")
)
```

---

### ğŸ“š 5. ThÃªm dá»¯ liá»‡u vÃ o DB

```python
collection.add(
    ids=["doc1", "doc2"],
    documents=["AI changes the world", "Cooking with friends is fun"]
)
```

ğŸ“Œ Tá»± Ä‘á»™ng táº¡o embeddings khi thÃªm dá»¯ liá»‡u.

---

### ğŸ” 6. Truy váº¥n dá»¯ liá»‡u

```python
results = collection.query(
    query_texts=["artificial intelligence"],
    n_results=2
)
print(results["documents"])
```

Káº¿t quáº£ tráº£ vá»:

* `ids`: ID tÃ i liá»‡u tÆ°Æ¡ng á»©ng
* `documents`: vÄƒn báº£n gá»‘c
* `distances`: khoáº£ng cÃ¡ch cosine

---

### ğŸ”„ 7. Cáº­p nháº­t / XÃ³a dá»¯ liá»‡u

```python
# Update
collection.update(ids=["doc1"], documents=["Updated text"])

# Upsert (thÃªm náº¿u chÆ°a cÃ³)
collection.upsert(ids=["doc3"], documents=["New document"])

# XÃ³a
collection.delete(ids=["doc2"])
```

---

### ğŸ§  8. Lá»c káº¿t quáº£ (Filtering)

ThÃªm metadata:

```python
collection.update(
    ids=["doc1"],
    metadatas=[{"type": "movie", "release_year": 2022}]
)
```

Lá»c khi query:

```python
results = collection.query(
    query_texts=["romantic comedies"],
    n_results=3,
    where={"type": "movie", "release_year": {"$gt": 2020}}
)
```

---

### ğŸ’¸ 9. Æ¯á»›c tÃ­nh chi phÃ­ embedding

```python
import tiktoken
enc = tiktoken.encoding_for_model("text-embedding-3-small")
tokens = sum(len(enc.encode(text)) for text in documents)
cost = 0.00002 * tokens / 1000
print(f"Tokens: {tokens}, Cost: ${cost:.5f}")
```

---

## ğŸš€ CHÆ¯Æ NG 4: XÃ‚Y Dá»°NG á»¨NG Dá»¤NG THá»°C Táº¾

---

### ğŸ’¬ A. Document Q&A System

**Má»¥c tiÃªu:** Tráº£ lá»i cÃ¢u há»i tá»« PDF.

```python
from pypdf import PdfReader
from transformers import pipeline

reader = PdfReader("policy.pdf")
text = "".join([page.extract_text() for page in reader.pages])

qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
print(qa(question="How many holidays?", context=text))
```

> á»¨ng dá»¥ng: Chatbot HR, phÃ¡p lÃ½, ná»™i quy cÃ´ng ty.

---

### ğŸ” B. Semantic Search + ChromaDB

* LÆ°u embeddings cá»§a vÄƒn báº£n.
* Khi query: embed cÃ¢u há»i â†’ tÃ¬m top N vÄƒn báº£n gáº§n nháº¥t.
* Cung cáº¥p láº¡i cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ â†’ tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n.

ğŸ’¡ ÄÃ¢y lÃ  nguyÃªn lÃ½ **RAG (Retrieval-Augmented Generation)**.

---

### ğŸ¯ C. Recommendation Engine

* Embed bÃ i viáº¿t ngÆ°á»i dÃ¹ng Ä‘Ã£ Ä‘á»c.
* TÃ­nh trung bÃ¬nh vector.
* Query ChromaDB Ä‘á»ƒ láº¥y top tÆ°Æ¡ng tá»±.

---

## ğŸ’¼ CHÆ¯Æ NG 5: CÃ‚U Há»I THá»°C Táº¾

| Chá»§ Ä‘á»                | CÃ¢u há»i                               | Tráº£ lá»i máº«u                                             |
| --------------------- | ------------------------------------- | ------------------------------------------------------- |
| **Embeddings**        | Embedding lÃ  gÃ¬?                      | Vector biá»ƒu diá»…n Ã½ nghÄ©a vÄƒn báº£n.                       |
| **Cosine similarity** | VÃ¬ sao dÃ¹ng cosine distance?          | So sÃ¡nh hÆ°á»›ng vector, bá» qua Ä‘á»™ dÃ i.                    |
| **Vector DB**         | Lá»£i Ã­ch cá»§a Vector DB?                | LÆ°u & truy váº¥n embedding hiá»‡u quáº£, má»Ÿ rá»™ng quy mÃ´.      |
| **ChromaDB**          | ChromaDB khÃ¡c Pinecone tháº¿ nÃ o?       | Chroma lÃ  open-source, Pinecone lÃ  dá»‹ch vá»¥ quáº£n lÃ½.     |
| **RAG**               | Retrieval-Augmented Generation lÃ  gÃ¬? | Káº¿t há»£p truy váº¥n vector + LLM Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n. |
| **á»¨ng dá»¥ng thá»±c táº¿**  | LÃ m sao xÃ¢y há»‡ thá»‘ng Q&A PDF?         | pypdf â†’ extract text â†’ embed â†’ ChromaDB â†’ query.        |

---
