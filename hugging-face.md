# ü§ñ HUGGING FACE & NLP

### (D√†nh cho AI / ML / NLP Engineer)

---

## üß© I. HUGGING FACE C∆† B·∫¢N

| Kh√°i ni·ªám            | M√¥ t·∫£                                                              | C√¢u tr·∫£ l·ªùi m·∫´u ph·ªèng v·∫•n                                                                       |
| -------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |
| **Hugging Face**     | N·ªÅn t·∫£ng m√£ ngu·ªìn m·ªü cho c·ªông ƒë·ªìng AI chia s·∫ª model, dataset, app. | ‚ÄúL√† n∆°i t·∫≠p trung m√¥ h√¨nh AI (NLP, CV, Audio), d·ªØ li·ªáu v√† API ph·ª•c v·ª• inference v√† hu·∫•n luy·ªán.‚Äù |
| **Hugging Face Hub** | Website n∆°i b·∫°n t√¨m ki·∫øm, t·∫£i v√† upload model/dataset.             | ‚ÄúHub cung c·∫•p model cards, dataset cards v√† API t√≠ch h·ª£p d·ªÖ d√†ng qua Python.‚Äù                   |
| **Th∆∞ vi·ªán ch√≠nh**   | `transformers`, `datasets`, `huggingface_hub`                      | ‚Äútransformers cho model, datasets cho d·ªØ li·ªáu, hub ƒë·ªÉ qu·∫£n l√Ω model/dataset.‚Äù                   |

---

## ‚öôÔ∏è II. TRANSFORMERS LIBRARY

### 1Ô∏è‚É£ `pipeline` ‚Äì C√°ch nhanh nh·∫•t ƒë·ªÉ ch·∫°y m√¥ h√¨nh

```python
from transformers import pipeline
pipe = pipeline("text-generation", model="openai-community/gpt2")
print(pipe("What if AI")[0]["generated_text"])
```

‚úÖ **C√¢u h·ªèi:**

> ‚Äúpipeline trong Hugging Face l√† g√¨?‚Äù

üí¨ **Tr·∫£ l·ªùi:**

> ‚ÄúL√† interface c·∫•p cao gi√∫p load model, tokenizer, v√† th·ª±c thi inference ch·ªâ b·∫±ng v√†i d√≤ng code.‚Äù

---

### 2Ô∏è‚É£ C√°c lo·∫°i pipeline ph·ªï bi·∫øn

| Task                       | M√¥ h√¨nh ti√™u bi·ªÉu                                 | ·ª®ng d·ª•ng                               |
| -------------------------- | ------------------------------------------------- | -------------------------------------- |
| `text-classification`      | `distilbert-base-uncased-finetuned-sst-2-english` | Ph√¢n lo·∫°i sentiment, spam              |
| `summarization`            | `sshleifer/distilbart-cnn-12-6`                   | T√≥m t·∫Øt vƒÉn b·∫£n                        |
| `question-answering`       | `distilbert-base-cased-distilled-squad`           | Tr·∫£ l·ªùi c√¢u h·ªèi theo ng·ªØ c·∫£nh          |
| `zero-shot-classification` | `facebook/bart-large-mnli`                        | Ph√¢n lo·∫°i vƒÉn b·∫£n kh√¥ng c·∫ßn hu·∫•n luy·ªán |
| `text-generation`          | `gpt2`, `mistral`, `llama-3`                      | Sinh vƒÉn b·∫£n, chatbot                  |

---

## üí¨ III. TEXT CLASSIFICATION

### üí° V√≠ d·ª•

```python
from transformers import pipeline
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
print(classifier("I love pineapple on pizza!"))
```

> Output ‚Üí `[{'label': 'POSITIVE', 'score': 0.99}]`

### üí≠ C√°c d·∫°ng classification:

| Lo·∫°i                         | M·ª•c ƒë√≠ch                              | V√≠ d·ª•                                     |
| ---------------------------- | ------------------------------------- | ----------------------------------------- |
| **Sentiment Analysis**       | C·∫£m x√∫c t√≠ch c·ª±c / ti√™u c·ª±c           | ‚ÄúBad service!‚Äù ‚Üí Negative                 |
| **Grammar Check**            | Ki·ªÉm tra ng·ªØ ph√°p                     | ‚ÄúHe eat pizza‚Äù ‚Üí Sai                      |
| **QNLI**                     | C√¢u tr·∫£ l·ªùi c√≥ kh·ªõp v·ªõi c√¢u h·ªèi kh√¥ng | ‚ÄúSeattle in Washington?‚Äù ‚úÖ                |
| **Zero-shot classification** | Ph√¢n lo·∫°i v√†o nh√≥m ch∆∞a h·ªçc           | VƒÉn b·∫£n ‚Üí ‚Äúsales‚Äù, ‚Äúsupport‚Äù, ‚Äúmarketing‚Äù |

> üí¨ **C√¢u h·ªèi:**
> ‚ÄúZero-shot classification kh√°c g√¨ supervised?‚Äù
> ‚úÖ **Tr·∫£ l·ªùi:** ‚ÄúZero-shot kh√¥ng c·∫ßn hu·∫•n luy·ªán nh√£n s·∫µn ‚Äì model d·ª±a v√†o ng·ªØ nghƒ©a c·ªßa nh√£n ƒë·ªÉ ph√¢n lo·∫°i.‚Äù

---

## üßæ IV. TEXT SUMMARIZATION

### ‚úçÔ∏è Kh√°i ni·ªám:

| Ki·ªÉu            | ƒê·∫∑c ƒëi·ªÉm                | ·ª®ng d·ª•ng                    |
| --------------- | ----------------------- | --------------------------- |
| **Extractive**  | Ch·ªçn c√¢u g·ªëc quan tr·ªçng | B√°o c√°o t√†i ch√≠nh, h·ª£p ƒë·ªìng |
| **Abstractive** | T·∫°o l·∫°i c√¢u m·ªõi c√¥ ƒë·ªçng | T√≥m t·∫Øt tin t·ª©c, blog       |

### üîπ V√≠ d·ª•:

```python
from transformers import pipeline
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
summary = summarizer("Data Science is a multidisciplinary field...", max_new_tokens=60)
print(summary[0]['summary_text'])
```

üí¨ **C√¢u h·ªèi:**

> ‚Äúƒêi·ªÉm kh√°c bi·ªát gi·ªØa extractive v√† abstractive summarization l√† g√¨?‚Äù
> ‚úÖ ‚ÄúExtractive ch·ªçn c√¢u g·ªëc, abstractive t·∫°o c√¢u m·ªõi ‚Äî abstractive t·ª± nhi√™n h∆°n nh∆∞ng ƒë√≤i h·ªèi t√†i nguy√™n cao h∆°n.‚Äù

---

## üìö V. DOCUMENT Q&A (Question Answering on PDFs)

### üìÑ Quy tr√¨nh:

1Ô∏è‚É£ D√πng `pypdf` ƒë·ªÉ ƒë·ªçc n·ªôi dung PDF
2Ô∏è‚É£ T·∫°o `pipeline("question-answering")`
3Ô∏è‚É£ Truy·ªÅn `context` = n·ªôi dung file, `question` = c√¢u h·ªèi

```python
from pypdf import PdfReader
from transformers import pipeline

reader = PdfReader("policy.pdf")
context = "".join([p.extract_text() for p in reader.pages])

qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
print(qa(question="How many holidays per year?", context=context)["answer"])
```

üí¨ **C√¢u h·ªèi:**

> ‚ÄúB·∫°n x√¢y d·ª±ng h·ªá th·ªëng h·ªèi ƒë√°p t√†i li·ªáu th·∫ø n√†o?‚Äù
> ‚úÖ ‚ÄúExtract text b·∫±ng `pypdf`, d√πng `QA pipeline` c·ªßa Hugging Face ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n context.‚Äù

---

## üß† VI. AUTOMODEL & AUTOTOKENIZER

### üîπ Khi c·∫ßn ki·ªÉm so√°t chi ti·∫øt h∆°n:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

custom_pipe = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
print(custom_pipe("This course is amazing!"))
```

### üí¨ So s√°nh nhanh:

| Ti√™u ch√≠        | `pipeline`       | `AutoModel` + `AutoTokenizer` |
| --------------- | ---------------- | ----------------------------- |
| M·ª©c ƒë·ªô ƒë∆°n gi·∫£n | D·ªÖ d√πng          | Ph·ª©c t·∫°p h∆°n                  |
| Linh ho·∫°t       | H·∫°n ch·∫ø          | To√†n quy·ªÅn ki·ªÉm so√°t          |
| D√πng khi n√†o?   | Demo, th·ª≠ nghi·ªám | Fine-tuning, production       |

---

## üß© VII. HUGGING FACE DATASETS

```python
from datasets import load_dataset
data = load_dataset("IVN-RIN/BioBERT_Italian", split="train")

# L·ªçc d·ªØ li·ªáu
filtered = data.filter(lambda row: "bella" in row['text'])
print(filtered)
```

üí¨ **C√¢u h·ªèi:**

> ‚ÄúHugging Face Datasets d√πng ƒë·ªãnh d·∫°ng g√¨?‚Äù
> ‚úÖ ‚ÄúApache Arrow ‚Äì l∆∞u d·ªØ li·ªáu d·∫°ng c·ªôt, gi√∫p truy v·∫•n nhanh v√† ti·∫øt ki·ªám b·ªô nh·ªõ.‚Äù

---

## üöÄ VIII. C√ÇU H·ªéI TH∆Ø·ªúNG G·∫∂P

| C√¢u h·ªèi                                      | C√°ch tr·∫£ l·ªùi ng·∫Øn g·ªçn                                         |
| -------------------------------------------- | ------------------------------------------------------------- |
| Hugging Face l√† g√¨?                          | C·ªông ƒë·ªìng m√£ ngu·ªìn m·ªü chia s·∫ª model, dataset, c√¥ng c·ª• AI.     |
| pipeline trong transformers l√† g√¨?           | Interface gi√∫p ch·∫°y m√¥ h√¨nh nhanh v·ªõi c·∫•u h√¨nh s·∫µn.           |
| Kh√°c nhau gi·ªØa pipeline v√† AutoModel?        | pipeline = ti·ªán, AutoModel = linh ho·∫°t.                       |
| Zero-shot classification l√† g√¨?              | Ph√¢n lo·∫°i m√† model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán nh√£n s·∫µn.             |
| Summarization c√≥ m·∫•y lo·∫°i?                   | 2 lo·∫°i: Extractive v√† Abstractive.                            |
| L√†m sao ƒë·ªçc PDF v√† h·ªèi ƒë√°p n·ªôi dung?         | D√πng `pypdf` + `pipeline(question-answering)`.                |
| Datasets c·ªßa Hugging Face d√πng ƒë·ªãnh d·∫°ng g√¨? | Apache Arrow.                                                 |
| Transformer model x·ª≠ l√Ω ng√¥n ng·ªØ th·∫ø n√†o?    | D√πng c∆° ch·∫ø self-attention ƒë·ªÉ h·ªçc m·ªëi quan h·ªá gi·ªØa c√°c token. |

---
